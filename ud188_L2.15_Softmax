import numpy as np
# Function that takes as input a list of numbers, and returns
# the list of values given by the softmax function.

my_list=[]
my_list.append(2)
my_list.append(1)
my_list.append(-1)

def softmax(L):
    expL = np.exp(L)
    sumExpL = sum(expL)
    result = []
    for i in expL:
        result.append(i*1.0/sumExpL)
    return result
    
    # Note: The function np.divide can also be used here, as follows:
    # def softmax(L):
    #     expL = np.exp(L)
    #     return np.divide (expL, expL.sum())

print("my_list", my_list)
print("softmax(my_list) = ", softmax(my_list))


# Write a function that takes as input two lists Y, P,
# and returns the float corresponding to their cross-entropy.
def cross_entropy(Y, P):
    Y = np.float_(Y)
    P = np.float_(P)
    return -np.sum(Y * np.log(P) + (1 - Y) * np.log(1 - P))

Y1 = [1,1,0]
P1 = [0.9,0.8,0.1]
print("Y1 = ", Y1)
print("P1 = ", P1)
print("cross_entropy(Y1, P1) = ", cross_entropy(Y1, P1))   

Y2 = [0,0,1]
P2 = [0.9,0.8,0.1]
print("Y2 = ", Y2)
print("P2 = ", P2)
print("cross_entropy(Y2, P2) = ", cross_entropy(Y2, P2))   